---
title: "Result7_LowdimIndepGauss"
output:
  workflowr::wflow_html:
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This .Rmd file is to plot results for the experiment MR.ASH and the comparison methods listed below.

(1) `glmnet` R package: Ridge, Lasso, E-NET
(2) `ncvreg` R package: SCAD, MCP
(3) `L0Learn` R package: L0Learn
(4) `BGLR` R package: BayesB, Blasso (Bayesian Lasso)
(5) `susieR` R package: SuSiE (Sum of Single Effect)
(6) `varbvs` R package: VarBVS (Variational Bayes Variable Selection)

The experiment is based on the following simulation setting.

### Design setting

We sample the standard i.i.d. Gaussian measurement $X_{ij} \sim N(0,1)$ anda construct $X \in \mathbb{R}^p$ with $n = 1010$ and $p = 1000$.

### Signal setting

We consider the case reported by [False discoveries occur early on the Lasso path](https://projecteuclid.org/euclid.aos/1509436830).

The signal is $\beta_j = M$ for $j \in J_1$ and $\beta_j = 1/M$ for $j \in J_2$, where $J_1$ and $J_2$ are mutually exclusive random subset of $\{1,\cdots,p\}$ of size $10$ and $90$, respectively. Otherwise $\beta_j = 0$.

$M = 1000$ is very large positive constant, and we fix $\sigma^2 = 1$.

### PVE

Then we sample $y = X\beta + \epsilon$, where $\epsilon \sim N(0,\sigma^2 I_n)$.

PVE is the proportion of variance explained, defined by

$$
{\rm PVE} = \frac{\textrm{Var}(X\beta)}{\textrm{Var}(X\beta) + \sigma^2},
$$

where $\textrm{Var}(a)$ denotes the sample variance of $a$ calculated using R function `var`. To this end, we set $\sigma^2 = \textrm{Var}(X\beta)$.

We do not fix PVE, but it will be extremely large since $M$ is large compared to $\sigma^2 = 1$.

### Performance Measure

The above two figures display the prediction error. The prediction error we define here is

$$
\textrm{Pred.Err}(\hat\beta;y_{\rm test}, X_{\rm test}) = \frac{\textrm{RMSE}}{\sigma} = \frac{\|y_{\rm test} - X_{\rm test} \hat\beta \|}{\sqrt{n}\sigma}
$$
where $y_{\rm test}$ and $X_{\rm test}$ are test data sample in the same way. If $\hat\beta$ is fairly accurate, then we expect that $\rm RMSE$ is similar to $\sigma$. Therefore in average $\textrm{Pred.Err} \geq 1$ and the smaller the better.

### Packages / Libraries

A list of packages we have loaded is collapsed. Please click "code" to see the list.

```{r library, message = FALSE}
library(Matrix); library(ggplot2); library(cowplot); library(susieR); library(BGLR);
library(glmnet); library(varbvs2); library(ncvreg); library(L0Learn); library(varbvs);
standardize = FALSE
source('code/method_wrapper.R')
source('code/sim_wrapper.R')
```

## Results

The result is summarized below.

```{r fig1, fig.height=7, fig.width=15}
res_df       = readRDS("results/subogdancandes.RDS")
method_list  = c("Mr.ASH","VarBVS","BayesB","Blasso","SuSiE","E-NET","Lasso","Ridge","SCAD","MCP","L0Learn")
for (i in 1:2) {
res_df[[i]]$fit   = rep(method_list, each = 20)
res_df[[i]]$fit   = factor(res_df[[i]]$fit, levels =  c("Mr.ASH","E-NET","Lasso","Ridge",
                                            "SCAD","MCP","L0Learn",
                                            "VarBVS","BayesB","Blasso","SuSiE"))
res_df[[i]] = res_df[[i]][res_df[[i]]$fit != "Ridge",]
}

p1 = my.box(res_df[[1]], "fit", "pred", values = gg_color_hue(11)[c(1,3,7,6,9,11,2,4,5,8)]) +
  theme(axis.line    = element_blank(),
        axis.text.x  = element_text(angle = 45,hjust = 1),
        legend.position = "none") +
  geom_hline(yintercept = median(res_df[[1]]$pred[res_df[[1]]$fit == "Mr.ASH"]), col = gg_color_hue(11)[1],
             linetype = "dotted", size = 1.5) +
  scale_y_continuous(trans = "log10")
p2 = my.box(res_df[[2]], "fit", "pred", values = gg_color_hue(11)[c(1,3,7,6,9,11,2,4,5,8)]) +
  theme(axis.line    = element_blank(),
        axis.text.x  = element_text(angle = 45,hjust = 1),
        legend.position = "none") +
  geom_hline(yintercept = median(res_df[[2]]$pred[res_df[[2]]$fit == "Mr.ASH"]), col = gg_color_hue(11)[1],
             linetype = "dotted", size = 1.5) +
  scale_y_continuous(trans = "log10")
subtitle  = ggdraw() + draw_label("Scenario: SuBogdanCandes, n = 1010, p = 1000, pve = 0.5", fontface  = 'bold', size = 18) 
p1        = plot_grid(subtitle, p1, ncol = 1, rel_heights = c(0.06,0.95))
subtitle  = ggdraw() + draw_label("Scenario: SuBogdanCandes, n = 1010, p = 1000, pve = 0.5", fontface  = 'bold', size = 18) 
p2        = plot_grid(subtitle, p2, ncol = 1, rel_heights = c(0.06,0.95))
#p0        = ggplot() + geom_blank() + theme_cowplot() + theme(axis.line = element_blank())
fig_main  = plot_grid(p1,p2, nrow = 1, rel_widths = c(0.3,0.3,0.3,0.3))
title     = ggdraw() + draw_label("Prediction Error (log-scale)", fontface = 'bold', size = 20) 
fig       = plot_grid(title,fig_main, ncol = 1, rel_heights = c(0.1,0.95))
fig
```

```{r fig2, fig.height=7, fig.width=15}
p1 = my.box(res_df[[1]], "fit", "time", values = gg_color_hue(11)[c(1,3,7,6,9,11,2,4,5,8)]) +
  theme(axis.line    = element_blank(),
        axis.text.x  = element_text(angle = 45,hjust = 1),
        legend.position = "none") +
  scale_y_continuous(trans = "log10")
p2 = my.box(res_df[[2]], "fit", "time", values = gg_color_hue(11)[c(1,3,7,6,9,11,2,4,5,8)]) +
  theme(axis.line    = element_blank(),
        axis.text.x  = element_text(angle = 45,hjust = 1),
        legend.position = "none") +
  scale_y_continuous(trans = "log10")
subtitle  = ggdraw() + draw_label("Scenario: SuBogdanCandes, n = 1010, p = 1000, pve = 0.5", fontface  = 'bold', size = 18) 
p1        = plot_grid(subtitle, p1, ncol = 1, rel_heights = c(0.06,0.95))
subtitle  = ggdraw() + draw_label("Scenario: SuBogdanCandes, n = 1010, p = 1000, pve = 0.5", fontface  = 'bold', size = 18) 
p2        = plot_grid(subtitle, p2, ncol = 1, rel_heights = c(0.06,0.95))
#p0        = ggplot() + geom_blank() + theme_cowplot() + theme(axis.line = element_blank())
fig_main  = plot_grid(p1,p2, nrow = 1, rel_widths = c(0.3,0.3,0.3,0.3))
title     = ggdraw() + draw_label("Computation Time (log-scale)", fontface = 'bold', size = 20) 
fig       = plot_grid(title,fig_main, ncol = 1, rel_heights = c(0.1,0.95))
fig
```

```{r eval = FALSE}
tdat1        = list()
n            = 1010
p            = 1000
s_list       = c(20,100)
method_list  = c("varbvs","bayesb","blasso","susie","enet","lasso","ridge","scad2","mcp2","l0learn")
method_num   = length(method_list) + 1
iter_num     = 20
pred         = matrix(0, iter_num, method_num); colnames(pred) = c("mr.ash", method_list)
time         = matrix(0, iter_num, method_num); colnames(time) = c("mr.ash", method_list)


for (iter in 1:2) {
  s               = s_list[iter]
for (i in 1:20) {
  set.seed(2010 + i)
  beta            = double(p)
  ind             = sample(p,s)
  M               = 1000
  beta[ind]       = 1/M
  beta[ind[1:10]] = M
  data            = simulate_data(n, p, s = s, seed = i, beta = beta, sigma = 1)
 
  for (j in 1:length(method_list)) {
    fit.method    = get(paste("fit.",method_list[j],sep = ""))
    fit           = fit.method(data$X, data$y, data$X.test, data$y.test, seed = i)
    pred[i,j+1]   = fit$rsse / data$sigma / sqrt(n)
    time[i,j+1]   = fit$t
  }
 
  fit         = fit.mr.ash(data$X, data$y, data$X.test, data$y.test, seed = i,
                           sa2 = (2^((0:19)) - 1)^2)
  pred[i,1]   = fit$rsse / data$sigma / sqrt(n)
  time[i,1]   = fit$t
  print(c(fit$fit$pi), digits = 3)
}
print("----")
print(pred)
print(time)
print("----")
tdat1[[iter]] = data.frame(pred = c(pred), time = c(time), fit = rep(c("mr.ash", method_list), each = 20))
}
```

## System Configuration

Click the below Session Info.