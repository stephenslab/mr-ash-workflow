---
title: "Result1_Ridge"
output:
  workflowr::wflow_html:
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

The experiment is based on the following simulation setting.

### Design setting

We sample the standard i.i.d. Gaussian measurement $X_{ij} \sim N(0,1)$ anda construct $X \in \mathbb{R}^p$ with $n = 500$ and $p \in \{50,100,200,500,1000,2000\}$.

### Signal setting

We sample the i.i.d. normal coefficients $\beta_j \sim N(0,\sigma_\beta^2)$ for $j = 1,\cdots,p$, or $\beta \sim N(0,\sigma_\beta^2 I_p)$.

This signal will be called `normal`.

### PVE

We fix PVE = 0.5. The relative performance does not very much dependent on the PVE value.

## Methods

In what follows, we briefly describe the comparison methods.

### Optimal Ridge

Let us recall that we sample the i.i.d. normal coefficients $\beta_j \sim N(0,\sigma_\beta^2)$ for $j = 1,\cdots,p$, or $\beta \sim N(0,\sigma_\beta^2 I_p)$.

We expect that in this simulation setting, the ridge regression with the optimal tuning parameter $\lambda$ will perform the best.

$$
p(\beta|y,X,\sigma^2) \propto p(y|X,\beta,\sigma^2) p(\beta) \propto \exp\left( - \frac{1}{2\sigma^2} \|y - X\beta\|_2^2 - \frac{1}{2\sigma_\beta^2} \|\beta\|_2^2 \right)
$$

This implies that $p(\beta|y,X,\sigma^2)$ is again a multivariate normal distribution and thus the posterior mode is equal to the posterior mean. Thus the optimal $\lambda$ is $\sigma^2 / (n\sigma_\beta^2)$.

### Elastic Net

The `glmnet` R package provides an elastic net implementation. It seeks to minimize the following objective function.

$$
\frac{1}{2n} \| y - X\beta \|^2 + \lambda \left(\alpha \|\beta\|_1 + 0.5 (1 - \alpha) \|\beta\|_2^2 \right)
$$

$\lambda$ and $\alpha$ are tuning parameters. For a fixed $\alpha$ in $\{ 0.1 * (a-1) : a = 1,\cdots, 11$, we run `cv.glmnet` with the default setting to tune $\lambda$ by cross-validation. Then we select a best tuple of $\alpha$ and $\lambda$ that minimizes the cross-validation error.

### Packages / Libraries

A list of packages we have loaded is collapsed. Please click "code" to see the list.

```{r library, message = FALSE}
library(Matrix); library(ggplot2); library(cowplot); library(susieR); library(BGLR);
library(glmnet); library(mr.ash.alpha); library(ncvreg); library(L0Learn); library(varbvs);
standardize = FALSE
source('code/method_wrapper.R')
source('code/sim_wrapper.R')
```

## Results

The result is summarized below. 

```{r fig1, fig.height=7, fig.width=15}
res_df       = readRDS("results/ridge_pve0.5.RDS")
p_list       = c(50,100,200,500,1000,2000)
method_list  = c("Mr.ASH","VarBVS","BayesB","Blasso","SuSiE","E-NET","Lasso","Ridge","SCAD","MCP","L0Learn","Ridge.opt")
col          = gg_color_hue(13)[1:11]
sdat = data.frame()
for (i in 1:6) {
  sdat       = rbind(sdat, data.frame(pred = colMeans(matrix(res_df[[i]]$pred,20,12)),
                                      time = colMeans(matrix(res_df[[i]]$time,20,12)),
                                      p = p_list[i],
                                      fit = method_list))
}
sdat$fit    = factor(sdat$fit, levels =  c("Mr.ASH","E-NET","Lasso","Ridge",
                                           "SCAD","MCP","L0Learn",
                                           "VarBVS","BayesB","Blasso","SuSiE",
                                           "Ridge.opt"))
sdat1       = sdat[sdat$fit %in% c("Mr.ASH","E-NET","Lasso","Ridge","Ridge.opt"),]
sdat2       = sdat[sdat$fit %in% c("Mr.ASH","E-NET","SCAD","MCP","L0Learn","Ridge.opt"),]
sdat3       = sdat[sdat$fit %in% c("Mr.ASH","VarBVS","BayesB","Blasso","SuSiE","Ridge.opt"),]

p1 = ggplot(sdat1) + geom_line(aes(x = p, y = pred, color = fit)) +
  geom_point(aes(x = p, y = pred, color = fit, shape = fit), size = 2.5) +
  theme_cowplot(font_size = 14) +
  scale_x_continuous(trans = "log10", breaks = p_list) +
  labs(y = "predictior error (rmse / sigma)", x = "number of coefficients (p)") +
  theme(axis.line = element_blank(),
        plot.title = element_text(hjust = 0.5)) +
  scale_color_manual(values = c(col[c(1,2,3,4)],"gray50")) +
  scale_shape_manual(values = c(19,17,24,25,15)) +
  scale_y_continuous(trans = "log10", limits = c(1.04,1.46), breaks = c(1.1,1.2,1.3,1.4))
p2 = ggplot(sdat2) + geom_line(aes(x = p, y = pred, color = fit)) +
  geom_point(aes(x = p, y = pred, color = fit, shape = fit), size = 2.5) +
  theme_cowplot(font_size = 14) +
  scale_x_continuous(trans = "log10", breaks = p_list) +
  labs(y = "", x = "number of coefficients (p)") +
  theme(axis.line = element_blank(),
        plot.title = element_text(hjust = 0.5)) +
  scale_color_manual(values = c(col[c(1,2,5,6,7)],"gray50")) +
  scale_shape_manual(values = c(19,17,9,3,11,15)) +
  scale_y_continuous(trans = "log10", limits = c(1.04,1.46), breaks = c(1.1,1.2,1.3,1.4))
p3 = ggplot(sdat3) + geom_line(aes(x = p, y = pred, color = fit)) +
  geom_point(aes(x = p, y = pred, color = fit, shape = fit), size = 2.5) +
  theme_cowplot(font_size = 14) +
  scale_x_continuous(trans = "log10", breaks = p_list) +
  labs(y = "", x = "number of coefficients (p)") +
  theme(axis.line = element_blank(),
        plot.title = element_text(hjust = 0.5)) +
  scale_color_manual(values = c(col[c(1,8,9,10,11)],"gray50")) +
  scale_shape_manual(values = c(19,4,5,7,8,15)) +
  scale_y_continuous(trans = "log10", limits = c(1.04,1.46), breaks = c(1.1,1.2,1.3,1.4))
fig_main  = plot_grid(p1,p2,p3, nrow = 1, rel_widths = c(0.35,0.35,0.3))
title     = ggdraw() + draw_label("Prediction Error (log-scale)", fontface = 'bold', size = 20) 
subtitle  = ggdraw() + draw_label("Scenario: IndepGauss + Normal, n = 500, p = 50,100,200,500,1000,2000, pve = 0.5", fontface  = 'bold', size = 18) 
fig       = plot_grid(title,subtitle,fig_main, ncol = 1, rel_heights = c(0.1,0.06,0.95))
fig
```

```{r fig2, fig.height=7, fig.width=15, fig.align = "center"}
p4  = my.box2(sdat[sdat$fit != "Ridge.opt",], "fit", "time",
             gg_color_hue(13)[1:11]) +
  theme(axis.line    = element_blank(),
        axis.text.x  = element_text(angle = 45,hjust = 1),
        legend.position = "none") +
  scale_y_continuous(trans = "log10")
title     = ggdraw() + draw_label("Computation time (log-scale)", fontface = 'bold', size = 20) 
subtitle  = ggdraw() + draw_label("Scenario: IndepGauss + Normal, n = 500, p = 50,100,200,500,1000,2000, pve = 0.5", fontface  = 'bold', size = 18) +
  labs(y = "computation time (sec)")
p0        = ggplot() + geom_blank() + theme_cowplot() + theme(axis.line = element_blank())
fig_main  = plot_grid(p0,p4,p0, nrow = 1, rel_widths = c(0.3,0.6,0.3))
fig       = plot_grid(title,subtitle,fig_main, ncol = 1, rel_heights = c(0.1,0.06,0.95))
fig
```

## Source code

The source code will be popped up when you click `code` on the right side. 

```{r code, eval = FALSE}
setwd("..")
library(Matrix); library(ggplot2); library(cowplot); library(susieR); library(BGLR);
library(glmnet); library(mr.ash.alpha); library(ncvreg); library(L0Learn); library(varbvs);
standardize = FALSE
source('code/method_wrapper.R')
source('code/sim_wrapper.R')
tdat1        = list()
n            = 500
p_range      = c(50,100,200,500,1000,2000)
method_list  = c("varbvs","bayesb","blasso","susie","enet","lasso","ridge","scad","mcp","l0learn")
method_list  = c("enet","lasso","ridge")
method_list2 = c("mr.ash", method_list,"mr.ash.order", "mr.ash.init","enet2","lasso2","ridge2","ridge.opt")
method_num   = length(method_list2)
iter_num     = 20
pred         = matrix(0, iter_num, method_num);
time         = matrix(0, iter_num, method_num);
colnames(pred) <- colnames(time) <- method_list2

for (iter in 1:6) {
p               = p_range[iter]
for (i in 1:20) {
  data          = simulate_data(n, p, s = p, seed = i, signal = "normal", pve = 0.5)
 
  for (j in 1:length(method_list)) {
    fit.method    = get(paste("fit.",method_list[j],sep = ""))
    fit           = fit.method(data$X, data$y, data$X.test, data$y.test, seed = i)
    pred[i,j+1]   = fit$rsse / data$sigma / sqrt(n)
    time[i,j+1]   = fit$t
    
    if (method_list[j] == "lasso") {
      lasso.path.order = mr.ash.alpha:::path.order(fit$fit$glmnet.fit)
      lasso.beta       = as.vector(coef(fit$fit))[-1]
      lasso.time       = c(fit$t, fit$t2)
    }
    
    if (method_list[j] == "lasso") {
      pred[i,method_num - 2] = fit$rsse2 / data$sigma / sqrt(n)
    } else if (method_list[j] == "enet") {
      pred[i,method_num - 3] = fit$rsse2 / data$sigma / sqrt(n)
    } else if (method_list[j] == "ridge") {
      pred[i,method_num - 1] = fit$rsse2 / data$sigma / sqrt(n)
    }
  }
 
  fit         = fit.mr.ash(data$X, data$y, data$X.test, data$y.test, seed = i,
                           sa2 = (2^((0:19) / 20) - 1)^2)
  pred[i,1]   = fit$rsse / data$sigma / sqrt(n)
  time[i,1]   = fit$t
  
  fit         = fit.mr.ash2(data$X, data$y, data$X.test, data$y.test, seed = i,
                            update.order = lasso.path.order,
                            sa2 = (2^((0:19) / 5) - 1)^2)
  pred[i,j+2] = fit$rsse / data$sigma / sqrt(n)
  time[i,j+2] = fit$t + lasso.time[2]
  
  fit         = fit.mr.ash2(data$X, data$y, data$X.test, data$y.test, seed = i,
                            beta.init = lasso.beta,
                            sa2 = (2^((0:19) / 5) - 1)^2)
  pred[i,j+3] = fit$rsse / data$sigma / sqrt(n)
  time[i,j+3] = fit$t + lasso.time[1]
  
  fit         = fit.ridge.opt(data$X, data$y, data$X.test, data$y.test, data$sigma, seed = i)
  pred[i,method_num] = fit$rsse / data$sigma / sqrt(n)
  time[i,method_num] = -Inf
}
tdat1[[iter]] = data.frame(pred = c(pred), time = c(time),
                           fit = rep(method_list2, each = 20))
}
```

## System Configuration

Click the below Session Info.