---
title: "Result11_UpdateOrder"
output:
  workflowr::wflow_html:
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This .Rmd file is to plot results for the experiment of MR.ASH's internal setting.

A list of update orders we consider is as follows.

(1) `random`: In each outer loop iteration, we sample a permuation map $\xi:\{1,\cdots,p\} \to \{1,\cdots,p\}$ uniformly at random and run inner loop iterations with the order based on $\xi$.

(2) `increasing`: $(1,\cdots,p)$, i.e. in each outer loop iteration, we update $q_1,q_2,\cdots,q_p$ in this order.

(3) `lasso.pathorder`: we update $q_j$ prior to $q_{j'}$ when $\beta_j$ appears earlier than $\beta_{j'}$ in the lasso path. If ties occur, then `increasing` order applies to those ties.

(4) `scad.pathorder`: we update $q_j$ prior to $q_{j'}$ when $\beta_j$ appears earlier than $\beta_{j'}$ in the scad path. If ties occur, then `increasing` order applies to those ties.

(5) `lasso.absorder`: we update $q_j$ prior to $q_{j'}$ when the lasso solution satisfies $\hat\beta_j \geq \hat\beta_{j'}$. If ties occur, then `increasing` order applies to those ties.

(6) `univar.absorder`: we update $q_j$ prior to $q_{j'}$ when the univariate linear regression solution satisfies $\hat\beta_j \geq \hat\beta_{j'}$. If ties occur, then `increasing` order applies to those ties.

### Design setting

We sample the equicorrelated Gaussian measurement $X_{ij} \sim N(0,\Sigma)$ where $\Sigma$ has diagonal entries $1$ and off-diagonal entries $\rho$. 

The we construct $X \in \mathbb{R}^p$ with $n = 500$ and $p = 2000$.

### rho

We will use $\rho = 0.95$. In this case, MR.ASH with a default setting (i.e. the `null` intiailization and the `increasing` update order) does not outperform.

Indeed, using this simulation setting and $\rho > 0.5$, E-ENET, SCAD, and MCP performs better than MR.ASH.

### Signal setting

We sample the i.i.d. normal coefficients $\beta_j \sim N(0,\sigma_\beta^2)$ for $j \in J$ and $\beta_ j = 0$ otherwise, where $J$ is a set of randomly $s$ indices in $\{1,â‹¯,p\}$c hosen uniformly at random.

This signal will be called `sparsenormal`.

We fix $s = 20$ throughout this experiment.

### PVE

Then we sample $y = X\beta + \epsilon$, where $\epsilon \sim N(0,\sigma^2 I_n)$.

We fix PVE = 0.5, where PVE is the proportion of variance explained, defined by

$$
{\rm PVE} = \frac{\textrm{Var}(X\beta)}{\textrm{Var}(X\beta) + \sigma^2},
$$
where $\textrm{Var}(a)$ denotes the sample variance of $a$ calculated using R function `var`. To this end, we set $\sigma^2 = \textrm{Var}(X\beta)$.

### Performance Measure

The above two figures display the prediction error. The prediction error we define here is

$$
\textrm{Pred.Err}(\hat\beta;y_{\rm test}, X_{\rm test}) = \frac{\textrm{RMSE}}{\sigma} = \frac{\|y_{\rm test} - X_{\rm test} \hat\beta \|}{\sqrt{n}\sigma}
$$
where $y_{\rm test}$ and $X_{\rm test}$ are test data sample in the same way. If $\hat\beta$ is fairly accurate, then we expect that $\rm RMSE$ is similar to $\sigma$. Therefore in average $\textrm{Pred.Err} \geq 1$ and the smaller the better.

### Packages / Libraries

A list of packages we have loaded is collapsed. Please click "code" to see the list.

```{r library, message = FALSE}
library(Matrix); library(ggplot2); library(cowplot); library(susieR); library(BGLR);
library(glmnet); library(mr.ash.alpha); library(ncvreg); library(L0Learn); library(varbvs);
standardize = FALSE
source('code/method_wrapper.R')
source('code/sim_wrapper.R')
```

## Results 

```{r fig1.1, fig.height=13, fig.width=15}
res_df       = readRDS("../results/mcmc50020.RDS")
method_list  = c("Mr.ASH","BayesB","bayesc","MCMC","BayesB2","bayesc2","MCMC2","Mr.ASH.g","Mr.ASH.g.init")
method_level = c("Mr.ASH","Mr.ASH.g","Mr.ASH.g.init","BayesB","BayesB2","bayesc","bayesc2","MCMC","MCMC2")
df           = data.frame()
p_range      = c(20,50,200,500,2000,10000)
for (i in 1:6) {
res_df[[i]]$fit   = rep(method_list, each = 20)
df                = rbind(df, data.frame(pred = c(colMeans(matrix(res_df[[i]]$pred, 20, 9))),
                                         time2 = apply(matrix(res_df[[i]]$time, 20, 9), 2, median),
                                         time = c(colMeans(matrix(res_df[[i]]$time, 20, 9))),
                                         p    = p_range[i],
                                         fit  = method_list))
}
df$fit = factor(df$fit, levels = method_level)
col   = c(gg_color_hue(13)[1],gg_color_hue(13)[13],"darkblue","dodgerblue","darkgreen","limegreen","gold","orange","salmon","skyblue")
shape = c(19,17,24,25,9,3,11,4,5,7,8,1,2,6)[1:8]
df    = df[df$fit %in% c("Mr.ASH","Mr.ASH.g","BayesB","BayesB2","MCMC","MCMC2"),]
p1    = ggplot(df) + geom_line(aes(x = p, y = pred, color = fit)) +
  geom_point(aes(x = p, y = pred, color = fit, shape = fit), size = 2.5) +
  theme_cowplot(font_size = 14) +
  scale_x_continuous(trans = "log10", breaks = c(10,20,50,200,1000,2000)) +
  labs(y = "predictior error (rmse / sigma)", x = "number of coefficients (p)") +
  theme(axis.line = element_blank(),
        plot.title = element_text(hjust = 0.5),
        axis.text.x  = element_text(angle = 45,hjust = 1)) +
  scale_color_manual(values = col) +
  scale_shape_manual(values = shape) +
  scale_y_continuous(trans = "log10")
fig_main = p1
title     = ggdraw() + draw_label("Prediction Error (log-scale)", fontface = 'bold', size = 20) 
subtitle  = ggdraw() + draw_label("Scenario: IndepGauss + PointNormal, n = 500, p = 20-10000, s = 20, pve = 0.5", fontface  = 'bold', size = 18) 
fig1.1    = plot_grid(title,subtitle,fig_main, ncol = 1, rel_heights = c(0.05,0.05,0.95))
```

```{r fig1.2, fig.height=13, fig.width=15}
res_df       = readRDS("../results/mcmcecorr50020020.RDS")
res_df1      = readRDS("../results/equicorr50020020.RDS")
for (i in 1:6) {
  res_df[[i]][1:20,] = res_df1[[i]][1:20, ]
  res_df[[i]]        = rbind(res_df[[i]], res_df1[[i]][241:260,])
}
method_list  = c("Mr.ASH","BayesB","bayesc","MCMC","BayesB2","bayesc2","MCMC2","Mr.ASH.g","Mr.ASH.g.init","Mr.ASH.init")
method_level = c("Mr.ASH","Mr.ASH.g","Mr.ASH.init","Mr.ASH.g.init","BayesB","BayesB2","bayesc","bayesc2","MCMC","MCMC2")
rho_list     = c(0,0.3,0.6,0.9,0.95,0.99)
a            = c(1,2,3,4,5,6)
out = matrix(0,6,10)
for (i in 1:6) {
  out[i,] = colMeans(matrix(res_df[[a[i]]]$pred, 20, 10))
}
colnames(out) = method_list
ind = 1:10
out = out[,ind]

col   = c(gg_color_hue(13)[1],"orange",gg_color_hue(13)[13],"darkblue",
          "dodgerblue","darkgreen","limegreen","gold","orange","salmon","skyblue")
shape = c(19,17,24,25,9,3,11,4,5,7,8,1,2,6)[1:8]
df = data.frame(x = rep(1:length(rho_list),length(ind)), rho = rep(rho_list, length(ind)),
                pred = c(out), fit = rep(colnames(out), each = length(rho_list)))
df$fit = factor(df$fit, levels =  method_level)
df    = df[df$fit %in% c("Mr.ASH","Mr.ASH.init","Mr.ASH.g.init","BayesB","BayesB2","MCMC","MCMC2"),]
df$size = 0.5
df$size[1:length(rho_list)] = 1.2
fig_dummy = ggplot(df) + geom_line(aes(x = x, y = pred, color = fit), size = df$size) +
  geom_point(aes(x = x, y = pred, color = fit, shape = fit), size = 2.5) +
  theme_cowplot(font_size = 14) +
  scale_x_continuous(breaks = c(1,2,3,4,5,6),
                     labels = c("0","0.3","0.6","0.9","0.95","0.99")) +
  labs(y = "predictior error (rmse / sigma)", x = "correlation across columns of X (rho)") +
  theme(axis.line = element_blank(),
        plot.title = element_text(hjust = 0.5)) +
  scale_color_manual(values = col) +
  scale_shape_manual(values = shape) +
  scale_y_continuous(trans = "log10", breaks = c(1,1.05,1.1,1.15,1.2,1.25,1.3,1.4)) +
  coord_cartesian(ylim = c(1,1.3))
p1        = fig_dummy + theme(legend.position = "none")
fig_main  = p1
subtitle  = ggdraw() + draw_label("Scenario: EquiCorrGauss + Normal, n = 500, p = 200, s = 20, pve = 0.5", fontface  = 'bold', size = 14) 
fig1.2      = plot_grid(subtitle,fig_dummy, ncol = 1, rel_heights = c(0.05,0.95))
```

## Source Code

```{r code, eval = FALSE}
tdat1        = list()
n            = 500
s            = 20
p_range      = c(20,50,200,500,2000,10000,20000)
method_list  = c("bayesb","bayesc","mcmc")
method_list2 = c("mr.ash",method_list,"bayesb2","bayesc2","mcmc2","mr.ash.opt","mr.ash.opt.init")
method_num   = length(method_list2)
iter_num     = 20
pred         = matrix(0, iter_num, method_num); colnames(pred) = method_list2
time         = matrix(0, iter_num, method_num); colnames(time) = method_list2
u            = length(method_list)


for (iter in 1:6) {
  p               = p_range[iter]
  for (i in 1:20) {
    data          = simulate_data(n, p, s = s, seed = i, signal = "normal", pve = 0.5)
    
    for (j in 1:u) {
      fit.method    = get(paste("fit.",method_list[j],sep = ""))
      fit           = fit.method(data$X, data$y, data$X.test, data$y.test, seed = i)
      pred[i,j+1]   = fit$rsse / data$sigma / sqrt(n)
      time[i,j+1]   = fit$t
      fit           = fit.method(data$X, data$y, data$X.test, data$y.test, seed = i, burnIn = 10000, nIter = 30000)
      pred[i,j+u+1]   = fit$rsse / data$sigma / sqrt(n)
      time[i,j+u+1]   = fit$t
    }
    
    fit         = fit.mr.ash(data$X, data$y, data$X.test, data$y.test, seed = i,
                             sa2 = (2^((0:19) / 5) - 1)^2)
    pred[i,1]   = fit$rsse / data$sigma / sqrt(n)
    time[i,1]   = fit$t
    
    fit           = fit.mr.ash2(data$X, data$y, data$X.test, data$y.test, seed = i,
                                sa2 = c(0, 1 / s), sigma2 = data$sigma^2,
                                update.pi = FALSE, pi = c(1 - s/p, s/p),
                                beta.init = NULL, update.order = NULL)
    pred[i,method_num-1]  = fit$rsse / data$sigma / sqrt(n)
    time[i,method_num-1]  = fit$t
    
    fit         = fit.lasso(data$X, data$y, data$X.test, data$y.test, seed = i)
    lasso.path.order = mr.ash.alpha:::path.order(fit$fit$glmnet.fit)
    lasso.beta       = as.vector(coef(fit$fit))[-1]
    lasso.time       = c(fit$t, fit$t2)
    
    fit           = fit.mr.ash2(data$X, data$y, data$X.test, data$y.test, seed = i,
                                sa2 = c(0, 1 / s), sigma2 = data$sigma^2,
                                update.pi = FALSE, pi = c(1 - s/p, s/p),
                                beta.init = lasso.beta, update.order = NULL)
    pred[i,method_num]  = fit$rsse / data$sigma / sqrt(n)
    time[i,method_num]  = fit$t
    
    print(c(pred[i,]))
    print(c(time[i,]))
  }
  tdat1[[iter]] = data.frame(pred = c(pred), time = c(time), fit = rep(method_list2, each = 20))
}
```

```{r code2, eval = FALSE}
tdat2        = list()
n            = 500
p            = 2000
s            = 20
rho_list     = c(0,0.3,0.6,0.9,0.95,0.99)
method_list  = c("bayesb","bayesc","mcmc")
method_list2 = c("mr.ash",method_list,"bayesb2","bayesc2","mcmc2","mr.ash.opt","mr.ash.opt.g")
method_num   = length(method_list2)
iter_num     = 20
pred         = matrix(0, iter_num, method_num); colnames(pred) = method_list2
time         = matrix(0, iter_num, method_num); colnames(time) = method_list2
u            = length(method_list)


for (iter in 1:6) {
  rho             = rho_list[iter]
  for (i in 1:20) {
    data          = simulate_data(n, p, s = s, seed = i, signal = "normal", rho = rho,
                                  design = "equicorrgauss", pve = 0.5)
    
    for (j in 1:u) {
      fit.method    = get(paste("fit.",method_list[j],sep = ""))
      fit           = fit.method(data$X, data$y, data$X.test, data$y.test, seed = i)
      pred[i,j+1]   = fit$rsse / data$sigma / sqrt(n)
      time[i,j+1]   = fit$t
      fit           = fit.method(data$X, data$y, data$X.test, data$y.test, seed = i, burnIn = 10000, nIter = 30000)
      pred[i,j+u+1]   = fit$rsse / data$sigma / sqrt(n)
      time[i,j+u+1]   = fit$t
    }
    
    fit         = fit.mr.ash(data$X, data$y, data$X.test, data$y.test, seed = i,
                             sa2 = (2^((0:19) / 20) - 1)^2)
    pred[i,1]   = fit$rsse / data$sigma / sqrt(n)
    time[i,1]   = fit$t
    
    fit           = fit.mr.ash2(data$X, data$y, data$X.test, data$y.test, seed = i,
                                sa2 = c(0, 1 / s), sigma2 = data$sigma^2,
                                update.pi = FALSE, pi = c(1 - s/p, s/p),
                                beta.init = NULL, update.order = NULL)
    pred[i,method_num]  = fit$rsse / data$sigma / sqrt(n)
    time[i,method_num]  = fit$t
    
    fit         = fit.lasso(data$X, data$y, data$X.test, data$y.test, seed = i)
    lasso.path.order = mr.ash.alpha:::path.order(fit$fit$glmnet.fit)
    lasso.beta       = as.vector(coef(fit$fit))[-1]
    lasso.time       = c(fit$t, fit$t2)
    
    fit           = fit.mr.ash2(data$X, data$y, data$X.test, data$y.test, seed = i,
                                sa2 = c(0, 1 / s), sigma2 = data$sigma^2,
                                update.pi = FALSE, pi = c(1 - s/p, s/p),
                                beta.init = lasso.beta, update.order = NULL)
    pred[i,method_num]  = fit$rsse / data$sigma / sqrt(n)
    time[i,method_num]  = fit$t
    
    print(c(pred[i,]))
    print(c(time[i,]))
  }
  tdat2[[iter]] = data.frame(pred = c(pred), time = c(time), fit = rep(method_list2, each = 20))
}
```