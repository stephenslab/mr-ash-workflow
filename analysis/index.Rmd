---
title: "Home"
site: workflowr::wflow_site
output:
  workflowr::wflow_html:
    toc: false
editor_options:
  chunk_output_type: console
---

## Plots for the paper

[Plots](Plots_for_paper.html)

---------------------

## Results for external comparison

This vignette is to plot results for the experiment MR.ASH and the comparison methods listed below.

(1) `glmnet` R package: Ridge, Lasso, E-NET
(2) `ncvreg` R package: SCAD, MCP
(3) `L0Learn` R package: L0Learn
(4) `BGLR` R package: BayesB, Blasso (Bayesian Lasso)
(5) `susieR` R package: SuSiE (Sum of Single Effect)
(6) `varbvs` R package: VarBVS (Variational Bayes Variable Selection)

### Performance measure

The prediction error we define here is
$$
\textrm{Pred.Err}(\hat\beta;y_{\rm test}, X_{\rm test}) = \frac{\textrm{RMSE}(\hat\beta;y_{\rm test} - X_{\rm test})}{\sigma} = \frac{\|y_{\rm test} - X_{\rm test} \hat\beta \|}{\sqrt{n}\sigma}
$$
where $y_{\rm test}$ and $X_{\rm test}$ are test data sample in the same way. If $\hat\beta$ is fairly accurate, then we expect that $\rm RMSE$ is similar to $\sigma$. Therefore in average $\textrm{Pred.Err} \geq 1$ and the smaller the better.

Here RMSE is the root-mean-squared error, and 

### Default setting

Throughout the studies, the default setting is

(1) The number of samples $n = 500$ and the number of coefficients $p = 2000$.
(2) The number of nonzero coefficients $s = 20$ (sparsity)
(3) Design: `IndepGauss`, i.e. $X_{ij} \sim N(0,1)$
(4) Signal shape: `PointNormal`, i.e.
$$
\beta_j = \left\{ \begin{array}{ll} \textrm{a normal random sample }\sim N(0,1), & j \in S \\
0, & j \notin S \end{array} \right.
$$
where $S$ is an index set of nonzero coefficients, sampled uniformly at random from $\{1,\cdots,p\}$. We define $s = |S|$ is a sparsity level. If $s$ is smaller, then we say the signal $\beta$ is sparser.

(5) PVE (Proportion of Variance Explained): 0.5
Given $X$ and $\beta$, we sample $y = X\beta + \epsilon$, where $\epsilon \sim N(0,\sigma^2 I_n)$. In this case, PVE is defined by
$$
{\rm PVE} = \frac{\textrm{Var}(X\beta)}{\textrm{Var}(X\beta) + \sigma^2},
$$
where $\textrm{Var}(a)$ denotes the sample variance of $a$ calculated using R function `var`. To this end, we set $\sigma^2 = \textrm{Var}(X\beta)$.
(6) Update order: `increasing`, i.e. `(1,...,p) = 1:p` in this order.
(7) Initialization: `null` i.e. a vector of zeros.

## MR.ASH and the comparison methods

### Some special cases

(1) [Ridge case](Result1_Ridge.html).

The ridge case is when the default setting applies to all but $s = p$.
We observe that the relative performance is dependent on the value of $p$ (compared to $n$). Thus we consider $p = 50,100,200,500,1000,2000$ while $n = 500$ being fixed.

The message here is MR.ASH performs reasonably well, and the only penalized linear regression method that outperforms Ridge with cross-validation. However, let us mention that Blasso (Bayesian Lasso) performs the best, which uses a non-sparse Laplace prior on $\beta$. Also, BayesB performs better than MR.ASH, which uses a spike-and-slab prior on $\beta$.

(2) [Nonconvex case](Result2_Nonconvex.html).

This experiment is to compare the flexibility of convex/non-convex shrinkage operators (E-NET, SCAD, MCP, L0Learn) vs MR.ASH shrinkage operator. The default setting applies to all but the signal shape and PVE = 0.99.

The reason why we set PVE = 0.99, the cross-validated solution of the method (E-NET, SCAD or MCP) acheives a nearly minimum prediction error on the solution path, i.e. the cross-validated solution is nearly the best the method can achieve. Thus we conclude that the cross-validation is fairly accurate.

### Important parameters for high-dimensional regression settings

(3) [High-dimensionality $p$](Result4_Highdim.html).

This experiment is to compare performance of the methods when $p$ varies while the rest being fixed. Since the theory says (e.g. [Slope meets Lasso: improved oracle bounds and
optimality](https://arxiv.org/pdf/1605.08651.pdf) ) that the prediction error scales 

(4) [Sparsity $s$](Result5_SparseSignal.html).

(5) [PVE](Result6_PVE.html).

### Performance on different design settings

(6) [LowdimIndepGauss](Result7_LowdimIndepGauss.html)

(7) [RealGenotype](Result8_RealGenotype.html)

(8) [Changepoint](Result9_Changepoint.html)

(9) [EquicorrGauss](Result10_Correlation.html)

---------------------

## Results for internal comparison

[MR.ASH update order](Result11_UpdateOrder.html)

A list of the update orders we consider is as follows.

(1) `random`: In each outer loop iteration, we sample a permuation map $\xi:\{1,\cdots,p\} \to \{1,\cdots,p\}$ uniformly at random and run inner loop iterations with the order based on $\xi$.
(2) `increasing`: $(1,\cdots,p)$, i.e. in each outer loop iteration, we update $q_1,q_2,\cdots,q_p$ in this order.
(3) `lasso.pathorder`: we update $q_j$ prior to $q_{j'}$ when $\beta_j$ appears earlier than $\beta_{j'}$ in the lasso path. If ties occur, then `increasing` order applies to those ties.
(4) `scad.pathorder`: we update $q_j$ prior to $q_{j'}$ when $\beta_j$ appears earlier than $\beta_{j'}$ in the scad path. If ties occur, then `increasing` order applies to those ties.
(5) `lasso.absorder`: we update $q_j$ prior to $q_{j'}$ when the lasso solution satisfies $|\hat\beta_j| \geq |\hat\beta_{j'}|$. If ties occur, then `increasing` order applies to those ties.
(6) `univar.absorder`: we update $q_j$ prior to $q_{j'}$ when the univariate linear regression solution satisfies $|\hat\beta_j| \geq |\hat\beta_{j'}|$. If ties occur, then `increasing` order applies to those ties.

[MR.ASH initialization](Result12_Initialization.html)

A list of the initializations we consider is as follows.

[Parametrization](Result15_EstimationOfSigma.html)

An analysis for the update rule of $\sigma^2$, based on different parametrizations of the model.

---------------------

Our story is 

(1) the MR.ASH's shrinkage operator (from a normal mixture prior) is more flexible than other popular shrinkage operators,
which is a parametric form of $\lambda$ (and $\gamma$ if exists).

(2) VEB can do better than cross validation.


