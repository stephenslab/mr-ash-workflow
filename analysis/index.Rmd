---
title: "Home"
site: workflowr::wflow_site
output:
  workflowr::wflow_html:
    toc: false
editor_options:
  chunk_output_type: console
---

## Results

### MR.ASH vs penalized linear regression

This is for MR.ASH vs. penalized linear regression.

Our story is 

(1) the MR.ASH's shrinkage operator (from a normal mixture prior) is more flexible than other popular shrinkage operators,
which is a parametric form of $\lambda$ (and $\gamma$ if exists).

(2) VEB can do better than cross validation.

#### Performance of methods -- Ridge case

1. [Result 1](Result_1_Ridge.html)

#### Flexibility of convex/non-convex shrinkage operators (E-NET, SCAD, MCP, L0Learn) vs MR.ASH shrinkage operator.

2. [Result 2](Result2_Nonconvex.html)

## MR.ASH vs. all other methods.

This is to reproduce the results in the Experiment section.
Throughout the studies, we will fix $n = 500$.

The default setting is
(1) The number of coefficients $p = 2000$
(2) The number of nonzero coefficients $s = 20$ (sparsity)
(3) Design: IndepGauss
(4) Signal shape for nonzero coefficients: SparseNormal
(5) PVE: 0.5

We will change exactly one of the below and fix the rest.

$p$
Sparsity
Design
PVE

### Different $p$

### Sparsity

IndepGauss + SparseNormal, $n = 500$, $p = 2000$ and $s = 1,5,20,100,500,2000$.

3. [Result 5](Result5_SparseSignal.html)

### Different PVE

### Different Design

#### LowdimIndepGauss + SuBogdanCandes

4. [Result 7](Result7_LowdimIndepGauss.html)

#### RealGenotype

5. [Result 8](Result8_RealGenotype.html)