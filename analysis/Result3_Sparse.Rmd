---
title: "Result3_Sparse"
output:
  workflowr::wflow_html:
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

The experiment is based on the following simulation setting.

### Design setting

We sample the standard i.i.d. Gaussian measurement $X_{ij} \sim N(0,1)$ anda construct $X \in \mathbb{R}^p$ with $n = 500$ and $p \in \{50,100,200,500,1000,2000\}$.

### Signal setting

We sample the i.i.d. normal coefficients $\beta_j \sim N(0,\sigma_\beta^2)$ for $j = 1,\cdots,p$, or $\beta \sim N(0,\sigma_\beta^2 I_p)$.

This signal will be called `normal`.

### PVE

We fix PVE = 0.5. The relative performance does not very much dependent on the PVE value.

```{r fig3, fig.height=7, fig.width=15, fig.align = "center"}
res_df = readRDS("results/sparsity500200020.RDS")
out = matrix(0,8,15)
for (i in 1:8) {
  out[i,] = colMeans(matrix(res_df[[i]]$pred, 20, 15))
}
out = out[,c(1:11,15)]
colnames(out) = c("Mr.ASH","VarBVS","BayesB","Blasso","SuSiE","E-NET","Lasso","Ridge","SCAD","MCP","L0Learn","Mr.ASH.opt")
ind = c(1,6,7,8,9,10,11)
out = out[,ind]

col   = gg_color_hue(13)[1:7]
shape = c(19,17,24,25,9,3,11,4,5,7,8,1)[1:7]
df = data.frame(s = rep(s_range[1:8], length(ind)), pred = c(out), fit = rep(colnames(out), each = 8))
df$fit = factor(df$fit, levels =  c("Mr.ASH","E-NET","Lasso","Ridge",
                                    "SCAD","MCP","L0Learn",
                                    "VarBVS","BayesB","Blasso","SuSiE","Mr.ASH.opt"))
p1 = ggplot(df) + geom_line(aes(x = s, y = pred, color = fit)) +
  geom_point(aes(x = s, y = pred, color = fit, shape = fit), size = 2.5) +
  theme_cowplot(font_size = 14) +
  scale_x_continuous(trans = "log10", breaks = s_range) +
  labs(y = "predictior error (rmse / sigma)", x = "number of nonzero coefficients (s)") +
  theme(axis.line = element_blank(),
        plot.title = element_text(hjust = 0.5)) +
  scale_color_manual(values = col) +
  scale_shape_manual(values = shape) +
  scale_y_continuous(trans = "log10", breaks = c(1,1.1,1.2,1.3,1.4)) +
  coord_cartesian(ylim = c(1,sqrt(2))) +
  geom_point(aes(x = 500, y = 1.1942835)) +
  geom_text(aes(x = 500, y = 1.1942835, label="ridge.opt"), vjust = 2)
fig_main = p1
title     = ggdraw() + draw_label("Prediction Error (log-scale)", fontface = 'bold', size = 20) 
subtitle  = ggdraw() + draw_label("Scenario: IndepGauss + PointNormal, n = 1000, p = 500, s = 1-500, pve = 0.5", fontface  = 'bold', size = 18) 
fig       = plot_grid(title,subtitle,fig_main, ncol = 1, rel_heights = c(0.03,0.03,0.95))
fig
```

```{r fig4, fig.height=7, fig.width=15, fig.align = "center"}
df$rel = df$pred / df$pred[19:27]
p2 = ggplot(df) + geom_line(aes(x = s, y = rel, color = fit)) +
  geom_point(aes(x = s, y = rel, color = fit, shape = fit), size = 2.5) +
  theme_cowplot(font_size = 14) +
  scale_x_continuous(trans = "log10", breaks = s_range) +
  labs(y = "relative predictior error (rmse / rmse_lasso)", x = "number of nonzero coefficients (s)") +
  theme(axis.line = element_blank(),
        plot.title = element_text(hjust = 0.5)) +
  scale_color_manual(values = col) +
  scale_shape_manual(values = shape) +
  scale_y_continuous(trans = "log10", breaks = c(1,1.1,1.2,1.3,1.4)) +
  coord_cartesian(ylim = c(0.93,1.2)) +
  geom_point(aes(x = 500, y = 1.1942835 / df$pred[27])) +
  geom_text(aes(x = 500, y = 1.1942835 / df$pred[27], label="ridge.opt"), vjust = 2)
fig_main = p2
title     = ggdraw() + draw_label("Relative Prediction Error (log-scale)", fontface = 'bold', size = 20) 
subtitle  = ggdraw() + draw_label("Scenario: IndepGauss + PointNormal, n = 1000, p = 500, s = 1-500, pve = 0.5", fontface  = 'bold', size = 18) 
fig       = plot_grid(title,subtitle,fig_main, ncol = 1, rel_heights = c(0.03,0.03,0.95))
fig
```

## Source code

The source code will be popped up when you click `code` on the right side. 

```{r code, eval = FALSE}
tdat1        = list()
n            = 500
p            = 2000
s_range      = c(1,2,5,10,20,50,100,200,500,1000,2000)
method_list  = c("varbvs","bayesb","blasso","susie","enet","lasso","ridge","scad2","mcp2","l0learn")
method_list2 = c("mr.ash",method_list,"enet2","lasso2","ridge2","mr.ash.opt")
method_num   = length(method_list2)
iter_num     = 20
pred         = matrix(0, iter_num, method_num); colnames(pred) = method_list2
time         = matrix(0, iter_num, method_num); colnames(time) = method_list2


for (iter in 1:8) {
  s               = s_range[iter]
  for (i in 1:20) {
    data          = simulate_data(n, p, s = s, seed = i, signal = "normal", pve = 0.5)
    
    for (j in 1:length(method_list)) {
      fit.method    = get(paste("fit.",method_list[j],sep = ""))
      fit           = fit.method(data$X, data$y, data$X.test, data$y.test, seed = i)
      pred[i,j+1]   = fit$rsse / data$sigma / sqrt(n)
      time[i,j+1]   = fit$t
      
      if (method_list[j] == "lasso") {
        pred[i,method_num - 2] = fit$rsse2 / data$sigma / sqrt(n)
      } else if (method_list[j] == "enet") {
        pred[i,method_num - 3] = fit$rsse2 / data$sigma / sqrt(n)
      } else if (method_list[j] == "ridge") {
        pred[i,method_num - 1] = fit$rsse2 / data$sigma / sqrt(n)
      }
    }
    
    fit         = fit.mr.ash(data$X, data$y, data$X.test, data$y.test, seed = i,
                            sa2 = c(0,2^(0:18) / 2^15))
    pred[i,1]   = fit$rsse / data$sigma / sqrt(n)
    time[i,1]   = fit$t
    
    fit           = fit.mr.ash2(data$X, data$y, data$X.test, data$y.test, seed = i,
                                sa2 = c(0, 1 / s), sigma2 = data$sigma^2,
                                update.pi = FALSE, pi = c(1 - s/p, s/p),
                                beta.init = NULL, update.order = NULL)
    pred[i,method_num]  = fit$rsse / data$sigma / sqrt(n)
    time[i,method_num]  = fit$t
    cat(pred[i,method_num], " ", mean((data$beta[data$beta != 0]/data$sigma)^2), " ", 1/s, "\n")
    
    print(c(pred[i,]))
  }
  tdat1[[iter]] = data.frame(pred = c(pred), time = c(time), fit = rep(method_list2, each = 20))
}
```