---
title: "Explore DSC results"
author: "Youngseok Kim and Peter Carbonetto"
site: workflowr::wflow_site
output: workflowr::wflow_html
---

This brief analysis is intended mainly to illustrate how to use the
DSC results to compare the performance of different methods for linear
regression, and in particular the new multiple regression adaptive
shrinkage ("mr. ash") method.

```{r knitr, echo=FALSE}
knitr::opts_chunk$set(comment = "#",results = "hold",collapse = TRUE,
                      fig.align = "center")
```

Load packages
-------------

Load a few packages used in the analysis below.

```{r load-pkgs, warning=FALSE, message=FALSE}
library(tools)
library(dscrutils)
library(ggplot2)
library(cowplot)
```

Import DSC results
------------------

Use function "dscquery" from the dscrutils package to extract the main
results of the DSC. (In practice, since this query can be slow when
the DSC contains a large number of results, it is better to call
`dscquery` once, save the resulting data frame, then load the data
frame in subsequent R sessions.)

```{r query-dsc-results, eval=FALSE}
dsc <- dscquery("dsc/mr_ash",
                targets = c("simulate","simulate.s","simulate.sigma",
                            "fit","fit.lambda_est_method","fit.timing",
                            "score.err"))
dsc <- transform(dsc,
                 simulate              = factor(simulate),
                 simulate.s            = factor(simulate.s),
                 fit                   = factor(fit),
                 fit.lambda_est_method = factor(fit.lambda_est_method))
save(list = "dsc",file = "results/dsc.RData")
resaveRdaFiles("results/dsc.RData")
```

The data frame should contain results for 1,920 separate runs
("pipelines").

```{r load-dsc-results}
load("results/dsc.RData")
nrow(dsc)
```

Inspect DSC results
-------------------

First, we check whether the method for selecting the penalty strength
parameter via cross-validation has any impact on the performance of
the glmnet methods:

```{r plot-lambda-est-method-vs-error, fig.height=3, fig.width=7.5}
pdat <- subset(dsc,
               simulate.s == 100 &
               is.element(fit,c("ridge","lasso","elastic_net")))
ggplot(pdat,aes(x = fit.lambda_est_method,y = score.err)) +
  geom_boxplot(color = "black",outlier.size = 1,width = 0.5) +
  facet_wrap(~fit,nrow = 1,scales = "free") +
  theme_cowplot(font_size = 12) +
  theme(axis.text.x = element_text(angle = 45,hjust = 1)) +
  labs(x = "")
```

Using `lambda.min` appears to offer a very slight improvement, at
least for `simulate.s = 100`. (We should check for other settings of
`simulate.s` as well.)

In light of this, we use the `lambda.min` results for subsequent
comparisons. The following plot compares the prediction accuracy of
all methods, separately for each setting of `simulate.s`:

```{r plot-method-vs-error, fig.height=6, fig.width=8}
pdat <- subset(dsc,
               fit.lambda_est_method == "lambda.min" |
               is.na(fit.lambda_est_method))
ggplot(pdat,aes(x = fit,y = score.err,fill = fit)) +
  geom_boxplot(color = "black",outlier.size = 1,width = 0.6) +
  facet_wrap(~simulate.s,nrow = 2,scales = "free") +
  theme_cowplot(font_size = 12) +
  theme(axis.text.x = element_text(angle = 45,hjust = 1)) +
  labs(x = "method",y = "rsse")
```

In addition to comparing accuracy of the predictions, it is also
important to compare the effort taken to arrive at the predictions:

```{r plot-method-vs-runtime, fig.height=6, fig.width=8}
ggplot(pdat,aes(x = fit,y = fit.timing,fill = fit)) +
  geom_boxplot(color = "black",outlier.size = 1,width = 0.6) +
  facet_wrap(~simulate.s,nrow = 2,scales = "free") +
  theme_cowplot(font_size = 12) +
  theme(axis.text.x = element_text(angle = 45,hjust = 1)) +
  labs(x = "method",y = "runtime (s)")
```
